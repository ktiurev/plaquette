{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfef660",
   "metadata": {},
   "source": [
    "Notebook created: 2023-06-06 11:45:55  \n",
    "Generated from: docs/qec_tutorials/decoding/decdoing_nb.rst  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d9409",
   "metadata": {},
   "source": [
    "# The decoding problem\n",
    "\n",
    "Previously, we learned how to define a quantum error correcting code by making use of\n",
    "the stabilizer formalism. We also learned how one can use take non-destructive\n",
    "measurements of the code’s stabilizers to obtain a syndrome. We encouraged you to use\n",
    "the simulation package `plaquette` to input many stances of errors and learn what\n",
    "kind of syndrome should you expect given an error. A syndrome, as we also saw, is\n",
    "degenerate, which means that many different errors can give the same syndromes.\n",
    "\n",
    "Errors with the same shape as stabilizers and logical operators do not toggle any of\n",
    "the stabilizer measurements. In the case of the former, the error is trivial because\n",
    "applying a stabilizer to the state leaves it unchanged. The latter is not trivial and\n",
    "posses a serious issue: a logical operator has been applied to the code, and we’ll\n",
    "never be able to detect it! All of our future calculations with that logical\n",
    "qubit will return wrong results.\n",
    "\n",
    "Quantum Error Correction doesn’t only consist of encoding a few logical qubits into many\n",
    "physical qubits and detecting errors on them, but it\n",
    "also consists of obtaining a correction operator $ C $. This correction is also a\n",
    "tensor product of Pauli operators on multiple qubits.\n",
    "\n",
    "A correction $ C $ for an error $ E $ must fulfill the requirement that it\n",
    "toggles the same stabilizer measurements.\n",
    "\n",
    "$$\n",
    "C E \\lvert \\psi \\rangle = C E g_i \\lvert \\psi \\rangle = g_i (CE \\lvert \\psi \\rangle) \\quad \\forall i\n",
    "$$\n",
    "\n",
    "In this way, we can be sure that an error-afflicted state is once again a stabilized\n",
    "state. Suppose that the error commutes with the $ i $-th stabilizer, then the\n",
    "correction must also commute with that same stabilizer:\n",
    "\n",
    "$$\n",
    "C E \\lvert \\psi \\rangle = C E g_i \\lvert \\psi \\rangle = C g_i E \\lvert \\psi \\rangle = g_i (CE \\lvert \\psi \\rangle)\n",
    "$$\n",
    "\n",
    "Said, differently, if $ [g_i,E]=0 $ then it follows that $ [g_i,C]=0 $.\n",
    "\n",
    "And, if it anti-commutes with the $ j $-th stabilizer, the correction must also\n",
    "anti-commute with that stabilizer:\n",
    "\n",
    "$$\n",
    "C E \\lvert \\psi \\rangle = C E g_j \\lvert \\psi \\rangle = - C g_j E \\lvert \\psi \\rangle = g_j (CE \\lvert \\psi \\rangle)\n",
    "$$\n",
    "\n",
    "Which means that if if $ \\{g_i,E\\}=0 $ then $ \\{g_i,C\\}=0 $.\n",
    "\n",
    "A correction $ C $ to an error $ E $ is considered to be successful if one\n",
    "retrieves the original state of the system.\n",
    "\n",
    "$$\n",
    "C\\lvert \\psi' \\rangle = C E \\lvert \\psi \\rangle  = \\lvert \\psi \\rangle\n",
    "$$\n",
    "\n",
    "Conversely, it is considered to have failed if the correction applied to the error\n",
    "operator results in a logical error,\n",
    "\n",
    "$$\n",
    "C\\lvert \\psi' \\rangle = CE \\lvert \\psi \\rangle = \\bar{L} \\lvert \\psi \\rangle,\n",
    "$$\n",
    "\n",
    "for any logical operator $ \\bar{L} $ acting on the encoded qubit(s).\n",
    "\n",
    "**Decoding** is the process through which one obtains a correction operator for a\n",
    "specific syndrome in a code.\n",
    "\n",
    "In the last tutorial we explored the action of errors on a code and observed what\n",
    "syndrome was obtained. Now, we engage in the opposite process: we are given a syndrome\n",
    "on the planar code of distance $ 3 $ and we have to guess what type of error and on\n",
    "which qubit the error occured.\n",
    "\n",
    "First of all, we import the modules that we need from `plaquette`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d75f7",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import plaquette\n",
    "import numpy as np\n",
    "from plaquette.codes import LatticeCode\n",
    "from plaquette.errors import QubitErrorsDict\n",
    "from plaquette.visualizer import LatticeVisualizer\n",
    "from plaquette.device import Device, MeasurementSample\n",
    "from plaquette.circuit.generator import generate_qec_circuit\n",
    "from plaquette import pauli\n",
    "planar_3 = LatticeCode.make_planar(size=3, n_rounds=1)\n",
    "vis = LatticeVisualizer(planar_3)\n",
    "vis.draw_lattice(height=300)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e179dd2",
   "metadata": {},
   "source": [
    "If we have a syndrome, we can use our understanding of how the stabilizer measurements\n",
    "are toggled to make a guess about what correction operator will fulfill the previous\n",
    "requirements and apply it to the code.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Operators in `plaquette` are represented through the tableau formalism, which\n",
    "will be introduced in future blog-posts. For now, it suffices to mention that we\n",
    "are creating the correction as a `python` `dict` (which is how we created the\n",
    "errors in the previous in the previous post), and we will transform them into\n",
    "the correct form by using a function from `plaquette.pauli`.\n",
    "\n",
    "Consider the syndrome shown below. We decide to define a correction operator consisting\n",
    "of an $ X $ operator on the qubit with index $ 3`and a :math:`Z $ operator on\n",
    "the qubit with index $ 7 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c1916",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0])\n",
    "correction_dict = {3: \"X\", 7: \"Z\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5b40e",
   "metadata": {},
   "source": [
    "In the case of the following syndrome, we can choose any of the two following\n",
    "corrections:\n",
    "\n",
    "- $ X $ on the qubits with indices $ 6 $ and $ 9 $.  \n",
    "- $ X $ on the qubits with indices $ 8 $ and $ 11 $.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb3201",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
    "correction_dict = {6: \"X\", 9: \"X\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a632e4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
    "correction_dict = {8: \"X\", 11: \"X\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9820063",
   "metadata": {},
   "source": [
    "Both corrections commute with the same stabilizers and they both anti-commute with the\n",
    "same stabilizers. Actually, you can see that both corrections are equivalent, given\n",
    "that one of them is equal to the other times the product of the stabilizer\n",
    "$ X_{6}X_{8}X_{9}X_{11} $.\n",
    "\n",
    "We can begin to give a preliminary definition for what is the decoding problem: when\n",
    "decoding, we are looking for a chain of qubits that connects any two syndromes and we\n",
    "place errors of the same type between them. Because both our correction and *whatever*\n",
    "the error was hold the same commutation/anti-commutation relations with every\n",
    "stabilizer, the correction will return the state of the system to a stabilizer state!\n",
    "\n",
    "Now that we have set this *preliminary* definition, we can use it to start decoding\n",
    "some other problems. Consider, for example, the following syndrome and two corrections\n",
    "that we can choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e107aef",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "correction_dict = {0: \"Z\", 10: \"Z\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20402d9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "correction_dict = {5: \"Z\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba08cd",
   "metadata": {},
   "source": [
    "We just had a new syndrome for which we could apply (at least) two different\n",
    "corrections, and both of the corrections reproduce the syndrome (they\n",
    "commute/anti-commute with the same stabilizers)! So, given our last conclusion, we are\n",
    "lead to believe that both corrections are the same. However, we have to ask ourselves,\n",
    "are they really the same?\n",
    "\n",
    "In the previous example, the corrections were equally valid because one\n",
    "correction is equal to the product of the other correction times a stabilizer.\n",
    "But this is not the case for our last example. Now, the former correction\n",
    "($ Z_0 Z_{10} $) is equal to the product of the latter times the logical\n",
    "operator $ \\bar{Z} $! This means that one correction rightfully corrects the\n",
    "error, while the other one contributes to having a logical error.\n",
    "\n",
    "What tools do we have that may help us make a decision between two or more *seemly\n",
    "equally valid* corrections, in order to reduce the probabilities of choosing the one\n",
    "that gives a logical error?\n",
    "\n",
    "Consider a code where every error appears with probability $ p $. An error occurs\n",
    "on that code and a syndrome is obtained through the stabilizer measurements. In our\n",
    "efforts to correct it, we make two different assumptions as to what the error was:\n",
    "either the error $ E' $ or the error $ E'' $. We can calculate the\n",
    "probability of each of these two errors as:\n",
    "\n",
    "$$\n",
    "p(E) = (1-p)^{1-N} p ^ N,\n",
    "$$\n",
    "\n",
    "where $ N $ is the number of single-qubit errors contained in the error $ E $.\n",
    "For any value below $ p<0.5 $, the probability of an error increases as the amount\n",
    "of single-qubit errors decreases [Gimeno-Segovia, 2015]. This means that we are most\n",
    "likely to avoid a logical error if we always choose a correction that matches the\n",
    "error with less single-qubit errors. In our last example, this would be the correction\n",
    "with a single $ X $ on the qubit with index $ 5 $.\n",
    "\n",
    "Because of this new insight, we might want to re-define the decoding problem as:\n",
    "decoding consists of finding a correction operator $ C $ such that it reproduces\n",
    "the same syndrome as the error $ E $ in a way that minimizes the probability of\n",
    "obtaining a logical error. This can be done by creating sets of qubits and applying\n",
    "single-qubit Pauli operators on them to connect the toggled stabilizer measurement\n",
    "within the syndrome, and then choosing the set with the least amount of qubits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa19e0",
   "metadata": {},
   "source": [
    "## A deeper look into logical errors\n",
    "\n",
    "We might want to make a small parenthesis now to talk about logical operators and\n",
    "logical errors. In the past blog-post, we showed three different surface codes and\n",
    "showed how the logical operators are defined. These definitions of the logical operators\n",
    "may have seemed fixed to a specific set of single-qubit Pauli operators on the qubits.\n",
    "Consider, for example, the logical $ \\bar{Z} $ on a planar code of distance\n",
    "$ 3 $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687f4ff",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "correction_dict = {0: \"Z\", 5: \"Z\", 10: \"Z\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34956f5e",
   "metadata": {},
   "source": [
    "It seems as though the logical $ \\bar{Z} $ is given by applying $ Z $ operators\n",
    "along a straight horizontal line at the bottom qubits of the code. However, if we\n",
    "multiply this operator times the stabilizer $ Z_{8}Z_{10}Z_{11} $, we are still\n",
    "applying the same logical operator on the code, but the shape of the chain of operators\n",
    "seems a little bit different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40baa4b0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "correction_dict = {0: \"Z\", 5: \"Z\", 8: \"Z\", 11: \"Z\"}\n",
    "correction_operator = pauli.dict_to_pauli(correction_dict, planar_3.n_data_qubits)\n",
    "vis.draw_latticedata(height=300, correction = [correction_operator])  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2a623",
   "metadata": {},
   "source": [
    "This is also a logical operator, and its effect is the same as the logical operator shown\n",
    "before. This means that a logical error is not necessarily restricted to the shape of\n",
    "the straight lines, but it can take many different paths along the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a167f",
   "metadata": {},
   "source": [
    "# The decoding graph\n",
    "\n",
    "A graph is a mathematical abstract object consisting of two types of elements: vertices\n",
    "(or nodes) and edges. Vertices can be imagined as points in an abstract space, and an\n",
    "edge is a line connecting two vertices. The edges in a graph allow us to find relations\n",
    "between different vertices in the graph.\n",
    "\n",
    "This last definition of the decoding problem can be redefined as a problem in graph\n",
    "theory. We have an underlying graph that represents the structure of the\n",
    "error-correcting code. In this graph, the vertices represent the ancillas used for\n",
    "measurements. In some specific scenarios, as we will later see, additional vertices\n",
    "should be added. An edge in the decoding graph represents the action of a single-qubit\n",
    "Pauli error on a given qubit, and the vertices connected by this edge are the\n",
    "stabilizer measurements that would be toggled in case that this error takes place.\n",
    "\n",
    "The toggled stabilizers, that is, the elements with a value of $ 1 $ in the\n",
    "syndrome, are identified in the graph as *syndrome vertices*, and the decoding problem\n",
    "is transformed to: find a combination of edges that connects pairs of syndrome vertices\n",
    "in such a way that the number of edges is minimized.\n",
    "\n",
    "Take, for example, the planar code of distance $ 3 $. This planar code has\n",
    "$ 12 $ measurement ancillas, which means that the graph is composed by 12 vertices.\n",
    "Each vertex is identified by giving it an index which indicates the ancilla it\n",
    "represents. The planar code has $ 13 $ data-qubits, and two different types of\n",
    "errors can act on each of the qubits (see note below). Thus, there are $ 26 $\n",
    "edges, $ 13 $ for the $ X $ errors and $ 13 $ for the $ Z $ errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9737fbb",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])  # little hack to \"highlight\" the syndrome nodes/vertices\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392181e",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">Actually there are 3 errors, if we take the $ Y $ Pauli operator into account;\n",
    "but this last operator can be separated into a $ X $ and a $ Z $. Hence, an\n",
    "error of the $ Y $-type is represented by the simultaneous action of both edges.\n",
    "\n",
    "In the planar code, we notice that the $ X $ errors toggle the following stabilizer\n",
    "measurements:\n",
    "\n",
    "- $ X_0 $: toggles the $ 0 $-th ancilla.  \n",
    "- $ X_1 $: toggles the $ 0 $-th and $ 1 $-st ancillas.  \n",
    "- $ X_2 $: toggles the $ 1 $-th ancilla.  \n",
    "- $ X_3 $: toggles the $ 0 $-th and $ 5 $-th ancillas.  \n",
    "- $ X_4 $: toggles the $ 1 $-st and $ 6 $-th ancillas.  \n",
    "- $ X_5 $: toggles the $ 5 $-th ancilla.  \n",
    "- $ X_6 $: toggles the $ 5 $-th and $ 6 $-th ancillas.  \n",
    "- $ X_7 $: toggles the $ 6 $-th ancilla.  \n",
    "- $ X_8 $: toggles the $ 5 $-th and $ 10 $-th ancillas.  \n",
    "- $ X_9 $: toggles the $ 6 $-th and $ 11 $-th ancillas.  \n",
    "- $ X_10 $: toggles the $ 10 $-th ancilla.  \n",
    "- $ X_11 $: toggles the $ 10 $-th and $ 11 $-th ancillas.  \n",
    "- $ X_12 $: toggles the $ 11 $-th ancilla.  \n",
    "\n",
    "\n",
    "In the case of the $ Z $ errors:\n",
    "\n",
    "- $ Z_0 $: toggles the $ 2 $-nd ancilla.  \n",
    "- $ Z_1 $: toggles the $ 3 $-rd ancilla.  \n",
    "- $ Z_2 $: toggles the $ 4 $-th ancilla.  \n",
    "- $ Z_3 $: toggles the $ 2 $-nd and $ 3 $-rd ancillas.  \n",
    "- $ Z_4 $: toggles the $ 3 $-rd and $ 4 $-th ancillas.  \n",
    "- $ Z_5 $: toggles the $ 2 $-nd and $ 7 $-th ancillas.  \n",
    "- $ Z_6 $: toggles the $ 3 $-rd and $ 8 $-th ancillas.  \n",
    "- $ Z_7 $: toggles the $ 4 $-th and $ 9 $-th ancillas.  \n",
    "- $ Z_8 $: toggles the $ 7 $-th and $ 8 $-th ancillas.  \n",
    "- $ Z_9 $: toggles the $ 8 $-th and $ 9 $-th ancillas.  \n",
    "- $ Z_10 $: toggles the $ 7 $-th ancilla.  \n",
    "- $ Z_11 $: toggles the $ 8 $-th ancilla.  \n",
    "- $ Z_12 $: toggles the $ 9 $-th ancilla.  \n",
    "\n",
    "\n",
    "In the following example we have four syndrome vertices which are the ones with\n",
    "indices: $ 2 $, $ 7 $, $ 10 $, $ 11 $. From our list of edges, we\n",
    "notice that we have the following two edges: $ (2, 7) $ and $ (10, 11) $. So,\n",
    "we choose these two edges as our correction. We can now take a look at our look-up\n",
    "table of edges and see that they represent the operators $ Z_5 $ and $ X_11 $,\n",
    "which is our correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229a990",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1])\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f253a2e",
   "metadata": {},
   "source": [
    "In this other example, we have one pair of syndrome vertices: $ 2 $ and $ 8 $.\n",
    "There is no edge that connects these two vertices together. However, we can follow a\n",
    "path of *connected edges* (i.e., edges that share one vertex) to *walk* from one vertex\n",
    "to the other. We choose the edges $ (2, 3) $ and $ (3, 8) $. Then, we say that\n",
    "our correction is given by the Pauli operators $ Z_3 $ and $ Z_6 $.\n",
    "\n",
    "We could have also chosen the edges $ (2, 7) $ and $ (7, 8) $, which give the\n",
    "correction $ Z_5Z_8 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93beb6d2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2cbc75",
   "metadata": {},
   "source": [
    "You may have noticed that here we have some single-qubit errors that only toggle one\n",
    "ancilla. We call these edges *dangling* edges, and we can add a new *virtual* vertex,\n",
    "which we can call an *open vertex*, and give it an index higher than the total amount\n",
    "of ancillas. By doing this, we can easily keep track of which one is the open vertex.\n",
    "The edge of the errors that toggle a single ancilla connect the vertex of the ancilla\n",
    "with the open vertex. In the case of the planar code of distance $ 3 $, this vertex\n",
    "would have the index $ 12 $.\n",
    "\n",
    "We can say that open vertices are *wild cards*. And we can treat them as syndrome\n",
    "vertices or not, depending on what is more convenient for us.\n",
    "\n",
    "Take a look at the following syndrome. Here, we only have a single syndrome vertex, and\n",
    "we can’t pair it with any other syndrome vertex. In this case, we can use the open\n",
    "vertex as the second open vertex with which we make the pairing. Hence, the edge that\n",
    "we choose is $ (3, 12) $. This edge represents an error toggling only the third\n",
    "vertex, $ Z_1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63941dcd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba278d",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">Dangling edges currently do not appear in the visualizer.\n",
    "\n",
    "The open vertex can be paired with as many syndrome vertices as necessary. In one of\n",
    "the previous examples (shown below) we could have also chosen the combination of\n",
    "dangling edges: $ (2, 12) $ and $ (8, 12) $, by pairing each syndrome vertex\n",
    "with the open vertex. The correction is given by $ Z_0Z_{11} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85f9c5",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "syndrome = np.array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
    "vis.draw_latticedata(height=300, syndrome=syndrome)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138928b",
   "metadata": {},
   "source": [
    "## The weighted decoding graph\n",
    "\n",
    "In a more realistic scenario, we can’t expect that all qubits have the same error\n",
    "probabilities. Each qubit will have a different probability influenced by its\n",
    "environment, the way in which operators are applied on them, their interactions with other\n",
    "qubits, etc. If we have a deep knowledge about the device we are working on, then we\n",
    "can use this information to increase our chances of applying the right corrections on\n",
    "the code.\n",
    "\n",
    "Consider a code where each type of error on each qubit has a different error\n",
    "probability. We use the quantity $ p_i $ to represent the probability of the error\n",
    "represented by the $ i $-th edge. An error $ E $ can be described by an array\n",
    "of boolean variables, where the $ i $-th position indicates if the edge is or isn’t\n",
    "in the error (we use $ E_i=0 $ to say that the edge is not in the error, and\n",
    "$ E_i=1 $ to say that the edge is in the error). We can then compute the\n",
    "probability of the error chain [Dennis, 2002]:\n",
    "\n",
    "$$\n",
    "p(E) = \\prod_i (1-p_i) ^{1 - E_i} p_i ^ {E_i} = \\prod_i \\left(\\frac{p_i}{1-p_i}\\right) ^{E_i} (1-p_i)\n",
    "$$\n",
    "\n",
    "We can take the negative logarithm of this quantity:\n",
    "\n",
    "$$\n",
    "-\\ln{p(E)} = \\sum_i E_i \\left(-\\ln\\left(\\frac{p_i}{1-p_i}\\right)\\right) + \\sum_i \\left(-\\ln(1-p_i)\\right)\n",
    "$$\n",
    "\n",
    "We notice that the last sum at the end of the right hand side   of the previous equation does not\n",
    "depend on the shape of the error, i.e., does not depend on $ E_i $, so we can\n",
    "ignore it. We call the quantity being added,\n",
    "$ w_i = -\\ln\\left(\\frac{p_i}{1-p_i}\\right) $, the weight of the $ i $-th edge.\n",
    "\n",
    "The edge-weights are an additive quantity that tell us how likely is an error to occur.\n",
    "The lower the weight-sum, the higher the probability of an error. We can construct\n",
    "errors by adding edge by edge and compute its likelihood easily by adding the weight of\n",
    "the new edge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b3050",
   "metadata": {},
   "source": [
    "# Decoding algorithms\n",
    "\n",
    "Actually performing the decoding is the last and hardest step when protecting the information of a code. This\n",
    "process must be done quickly, faster than the emergence of new errors. It would be\n",
    "pointless to correct one error if, during the time required to do so, ten new errors\n",
    "appeared. Because of this, we need to automate this process. Because of this, a few\n",
    "decoding algorithms have been proposed.\n",
    "\n",
    "The decoding algorithms are, as their name suggests, algorithms that take as input the\n",
    "graph (or weighted graph) of the underlying structure of a code and a syndrome and give\n",
    "as output a selection of edges, i.e., a correction operator. These decoding algorithms\n",
    "should have, in a worst case scenario, a runtime that scales polynomially with the size\n",
    "of the code. The first decoder to achieve this is the Minimum Weight Perfect Matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8faf5",
   "metadata": {},
   "source": [
    "## The Minimum Weight Perfect Matching\n",
    "\n",
    "This algorithm consists of calling the following two algorithms [Higgot, 2021]:\n",
    "\n",
    "- Dijkstra’s algorithm [Dijkstra, 1959]: here, one computes the shortest path between\n",
    "  each pair of syndrome vertices and the path between each syndrome vertex and the open\n",
    "  vertex. The distance of each path is stored in a matrix, and the path is also stored.\n",
    "  When the graph does not contain weights, the distance between each pair is given by the\n",
    "  Manhattan distance between the two vertices (i.e., the sum of the absolute difference\n",
    "  between each of the coordinates of the ancillas supporting the vertices in the code’s\n",
    "  lattice). When the graph is weighted, then the distance is given by the sum of the\n",
    "  weights of the edges in the path.  \n",
    "- Kolmogorov’s Blossom V algorithm [Kolmogorov, 2009]: here, every combination of pairs\n",
    "  of syndrome vertices is created and the sum of the distances between each pair of the\n",
    "  vertices per combination is obtained. In the end, the algorithm chooses the combination\n",
    "  that has the smallest sum.  \n",
    "\n",
    "\n",
    "Once that the combination with the smallest distance-sum has been obtained, the\n",
    "algorithm obtains the edges making up the paths in that combination and returns it as\n",
    "the correction.\n",
    "\n",
    "This algorithm manages to find a correction with a runtime that scales polynomially\n",
    "with the number of qubits in the code, with a complexity of\n",
    "$ \\mathcal{O}(n^3 \\ln{(n)}) $. Among the decoders that run in polynomial time, the\n",
    "MWPM is the one with the highest decoding accuracy, i.e., the one that obtains logical\n",
    "errors less often.\n",
    "\n",
    "We can use `plaquette` and its integration with `PyMatching`’s\n",
    "implementation of the MWPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60164d73",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from plaquette.decoders import PyMatchingDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64693a18",
   "metadata": {},
   "source": [
    "We can now use the decoder to obtain the correction for a syndrome in a surface code.\n",
    "We will now simulate a distance $ 5 $ planar code to show corrections for more\n",
    "complicated errors on an unweighted graph. Here, we will give each qubit a probability\n",
    "of $ 0.04 $ for $ X $, for $ Y $ and for $ Z $ errors. In these\n",
    "examples (plural, because you can run the code-block multiple times and get a new,\n",
    "random error any time), you can visualize the syndrome and the correction.\n",
    "\n",
    "We are making use of the function `get_sample_random`, which is very similar to\n",
    "`get_syndrome_random` from the past tutorial, but now we are receiving the whole\n",
    "sample, which also includes a list of the erased qubits (for our current conditions,\n",
    "none are erased, but the decoder needs this information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933c224",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def get_sample_random(code, qed, logical_ops=\"X\"):\n",
    "    circuit = generate_qec_circuit(code, qed, {}, logical_ops)\n",
    "    dev = Device(\"clifford\")\n",
    "    dev.run(circuit)\n",
    "    raw_results, erasure = dev.get_sample()\n",
    "    sample = MeasurementSample.from_code_and_raw_results(code, raw_results, erasure)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a00ad",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "planar_5 = LatticeCode.make_planar(size=5, n_rounds=1)\n",
    "p = 0.04\n",
    "qed: QubitErrorsDict = {\n",
    "    \"pauli\": {i: dict(x=p, y=p, z=p) for i in range(planar_5.n_data_qubits)}\n",
    "}\n",
    "sample = get_sample_random(planar_5, qed)\n",
    "mwpm = PyMatchingDecoder.from_code(planar_5, qed, weighted=False)\n",
    "correction = [mwpm.decode(sample.erased_qubits, sample.syndrome)]\n",
    "vis_5 = LatticeVisualizer(planar_5)\n",
    "vis_5.draw_latticedata(height=500, syndrome=sample.syndrome[0], correction=correction)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8149b",
   "metadata": {},
   "source": [
    "We can also use `plaquette` to use the weighted variant of the MWPM. For this, we\n",
    "decide to give each qubit a random error probability between $ 0 $ and $ 0.1 $.\n",
    "\n",
    "We can also use `plaquette` to plot a heat map of the error probability for $ X $\n",
    "errors and for $ Z $ errors, as shown in the following blocks of code.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">The effective $ X $ error probability on a qubit is equal to the probability of\n",
    "having an $ X $ error plus the probability of having a $ Y $ error on that\n",
    "same qubit. Likewise, the effective $ Z $ error probability on a qubit is the\n",
    "sum of the $ Z $ error probability plus the $ Y $ error probability. We\n",
    "define a calculator `get_effective_probabilities` to handle these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd2de3",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def get_effective_probabilities(code, qed):\n",
    "    x_prob = np.zeros(code.n_data_qubits)\n",
    "    z_prob = np.zeros(code.n_data_qubits)\n",
    "    pauli_probs = qed.get(\"pauli\", {})\n",
    "    for qubit in pauli_probs.keys():\n",
    "        x_prob[qubit] += pauli_probs.get(qubit, {}).get(\"x\", 0)\n",
    "        x_prob[qubit] += pauli_probs.get(qubit, {}).get(\"y\", 0)\n",
    "        z_prob[qubit] += pauli_probs.get(qubit, {}).get(\"z\", 0)\n",
    "        z_prob[qubit] += pauli_probs.get(qubit, {}).get(\"y\", 0)\n",
    "    return x_prob, z_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110c30e",
   "metadata": {},
   "source": [
    "Now, we generate the random probability distribution, calculate the effective\n",
    "probabilities and create a visualizer that will allow us to see the probability\n",
    "distributions of each type of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41b5e0",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "rnd = np.random.default_rng(seed=1234)\n",
    "error_probabilities = rnd.uniform(0, 0.1, (planar_5.n_data_qubits, 3))\n",
    "random_qed: QubitErrorsDict = {\n",
    "    \"pauli\": {i: dict(x=error_probabilities[i, 0],\n",
    "                      y=error_probabilities[i, 1],\n",
    "                      z=error_probabilities[i, 2]) for i in range(planar_5.n_data_qubits)}\n",
    "}\n",
    "x_prob, z_prob = get_effective_probabilities(planar_5, random_qed)\n",
    "vis_5x = LatticeVisualizer(planar_5, qubit_error_probs=x_prob)\n",
    "vis_5z = LatticeVisualizer(planar_5, qubit_error_probs=z_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea092c40",
   "metadata": {},
   "source": [
    "We visualize the effective $ X $ error probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663db159",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vis_5x.draw_lattice(height=300)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b6d72",
   "metadata": {},
   "source": [
    "We visualize the effective $ Z $ error probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bf47e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "vis_5z.draw_lattice(height=300)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa960a",
   "metadata": {},
   "source": [
    "And now, we can run multiple times the following code block to obtain a random syndrome\n",
    "sample and the correction given by the MWPM decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f811e9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sample = get_sample_random(planar_5, random_qed)\n",
    "mwpm = PyMatchingDecoder.from_code(planar_5, random_qed, weighted=True)\n",
    "correction = [mwpm.decode(sample.erased_qubits, sample.syndrome)]\n",
    "vis_5 = LatticeVisualizer(planar_5)\n",
    "vis_5.draw_latticedata(height=500, syndrome=sample.syndrome[0], correction=correction)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8581850",
   "metadata": {},
   "source": [
    "## Union Find\n",
    "\n",
    "Following from the argument that decoding has to be done quickly so that we can keep up\n",
    "with the emergence of new errors, the Union Find is a new decoder that has been\n",
    "recently proposed, and it is able to find a correction in almost-linear time!\n",
    "\n",
    "This algorithm consists of two sub-algorithms:\n",
    "\n",
    "- The Maximum Likelihood decoder for erasures (also known as “peeling decoder”). This is\n",
    "  an algorithm that corrects for a specific type of error: the erasure. An erasure is an\n",
    "  error where a qubit has been physically lost or where the information of the qubit has\n",
    "  been erased. An erasure is well-located within the code. Whenever a qubit is erased, we\n",
    "  know with full certainty which one it is. The qubit is then replaced by *or reinitialised\n",
    "  as a qubit in a completely mixed state, i.e., the state of this qubit is in the\n",
    "  correct state with probability of $ 25\\% $ or it will have an $ X $, $ Y $\n",
    "  or $ Z $ error, each with a probability of $ 25\\% $ [Delfosse, 2020].  \n",
    "  Whenever we have an erasure (i.e., a set of erased qubits), we can create a sub-set\n",
    "  $ \\mathcal{R} $ of the edges supported by these qubits in the graph. We can do\n",
    "  this because the location of the erased qubits is well-known! We also perform our\n",
    "  stabilizer measurements to obtain the syndrome of a given error.  \n",
    "  The peeling decoder works as follows: first, the erasure is identified on the\n",
    "  decoding graph as a subset of edges. Second, we remove edges from the erasure in\n",
    "  such a way that the erasure sub-graph contains no loops (a path of edges that\n",
    "  returns to a previously visited vertex). This loop-less graph, called a *forest*,\n",
    "  should contain as many edges from the original erasure as possible. This process\n",
    "  can be done by choosing a seed (if the code has an open boundary, then the open\n",
    "  vertices are first chosen as seeds), and *walking* through the erasure by adding\n",
    "  edges if only one of its vertices is already in the forest. If both vertices are\n",
    "  already in the forest, the edge is discarded. If a connected sub-graph is not\n",
    "  connected to an open vertex, we can choose randomly any of its vertices as the\n",
    "  seed. The figure below shows how the forest is created in a graph of a planar code\n",
    "  of distance $ 4 $ (we are only showing the edges that represent $ Z $\n",
    "  errors for simplicity):  \n",
    "  <img src=\"qec_tutorials/decoding/forest.png\" style=\"width:300px;\">\n",
    "  \n",
    "  Finally, the forest is peeled from its *leaves*. We call a *leaf* an edge that is\n",
    "  connected to the rest of the erasure through only one of its vertices, while the\n",
    "  other vertex, which we call *pendant vertex* is disconnected from the erasure.  \n",
    "  When removing a leaf, if the pendant vertex **is not** a syndrome vertex, then we\n",
    "  continue and remove another leaf. If the pendant vertex **is** also a syndrome\n",
    "  vertex, then the edge is added to the correction operator. Then, the state of the\n",
    "  non-pendant vertex of the leaf is flipped: if the non-pendant vertex was a syndrome\n",
    "  vertex, then it will no longer be a syndrome vertex; if the non-pendant vertex\n",
    "  wasn’t a syndrome vertex, it will now become a syndrome vertex. Below you can find\n",
    "  an example of how the peeling is performed, and how the correction is obtained.  \n",
    "  <img src=\"qec_tutorials/decoding/peeling.png\" style=\"width:300px;\">\n",
    "  \n",
    "  This decoding algorithm works only for erasures. The extra information regarding\n",
    "  the location of the erasures is what gives this algorithm a linear complexity. One\n",
    "  of the decoder’s requirements is that every connected subgraph within the erasure\n",
    "  must have an even number of syndrome vertices **or** it must be connected to an\n",
    "  open vertex.  \n",
    "- Syndrome validation [Delfosse, 2021]. As we claimed, the peeling decoder works only\n",
    "  with erasures, because it requires a subgraph within the code’s decoding graph\n",
    "  containing the edges that correspond to erased qubits. The peeling decoder cannot be\n",
    "  used to correct for Pauli errors, because the only information that can be retrieved\n",
    "  from these errors is the syndrome. We are missing the erasure. Because of this, the\n",
    "  syndrome validation takes as input a pre-processed erasure (which can be empty) and a\n",
    "  syndrome and attempts to create a *virtual* erasure that fulfills the requirements set\n",
    "  by the peeling decoder: to have an erasure, and that each connected subgraph within the\n",
    "  erasure must have an even number of syndrome vertices or be connected to an open vertex.  \n",
    "  The syndrome validation works as follows: it first identifies within the graph the\n",
    "  real erasure (again, might be empty) as a subset of edges, and the syndrome\n",
    "  vertices within the syndrome graph. Then, it identifies *clusters*. A cluster is a\n",
    "  connected sub-graph within the erasure. A cluster can be as small as a single\n",
    "  vertex (which, in this case, would be a single syndrome vertex). Then, one creates\n",
    "  a *growth list*. Here, one identifies every cluster with an *odd parity*, where we\n",
    "  use the term odd parity to refer to clusters that have an odd number of syndrome\n",
    "  vertices **and** are **not** connected to an open vertex. Then, each one of the\n",
    "  clusters in this list grows (or *spreads*) towards every edge from the decoding\n",
    "  graph that is connected to itself. At a single growth step of a cluster, every edge\n",
    "  grows only by half.  \n",
    "  Whenever an edge with vertices $ \\{u, v\\} $ is fully grown, we call a function\n",
    "  `Find` on both vertices. This function tells us to which cluster do these vertices\n",
    "  belong to. If:  \n",
    "  - `Find(u)` or `Find(v)` returns nothing, then $ u $ (or $ v $) is not yet\n",
    "    contained in any cluster, and then it is added to the cluster from which the growth\n",
    "    was performed.  \n",
    "  - `Find(u) == Find(v)`, then both of the vertices belong to the same cluster.  \n",
    "  - `Find(u) != Find(v)`, these two vertices belong to different clusters. In this\n",
    "    case, we call a function `Union(u, v)` and these two clusters are merged into one.\n",
    "    Preferably, the smaller cluster is merged into the bigger one. The new cluster\n",
    "    (result of merging the two of them) may be removed from the growth list if it no\n",
    "    longer has an odd parity.  \n",
    "  This process of growth, call of `Find`, call of `Union`, is repeated until all\n",
    "  clusters have an *even parity*. At this point, we can say that the syndrome\n",
    "  validation is completed. Below you can find an example of a syndrome validation on\n",
    "  a code with an initial erasure and syndrome vertices. By the end, we have a new\n",
    "  erasure and the same syndrome.  \n",
    "  <img src=\"qec_tutorials/decoding/syndrome-validation.png\" style=\"width:300px;\">\n",
    "  \n",
    "  The complexity of this algorithm is almost-linear, namely,\n",
    "  $ \\mathcal{O}(n\\alpha(n)) $, where $ \\alpha(n) $ is the inverse of\n",
    "  Ackerman’s function, and its value is smaller or equal than $ 3 $ for any\n",
    "  practical value of $ n $.  \n",
    "\n",
    "\n",
    "The Union Find decoder is the algorithm that takes as input an erasure and a syndrome,\n",
    "then it processes them through the syndrome validation to obtain a new, virtual\n",
    "erasure, and then calls the peeling decoder by giving as input the virtual erasure and\n",
    "the syndrome to obtain a selection of edges as the correction.\n",
    "\n",
    "Naturally, the Union Find decoder is included in the library `plaquette`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdef9d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from plaquette.decoders import UnionFindDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c50dd",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">This decoder is Python-based. For a much more performant option, have a look\n",
    "at [its C++ version](https://github.com/qc-design/plaquette-unionfind).\n",
    "\n",
    "We will now use the Union Find to decode some samples. We will use the same code and\n",
    "error probabilities as we did for the MWPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a4e53",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sample = get_sample_random(planar_5, qed)\n",
    "uf = UnionFindDecoder.from_code(planar_5, qed, weighted=False)\n",
    "correction = [uf.decode(sample.erased_qubits, sample.syndrome)]\n",
    "vis_5 = LatticeVisualizer(planar_5)\n",
    "vis_5.draw_latticedata(height=500, syndrome=sample.syndrome[0], correction=correction)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387fc60",
   "metadata": {},
   "source": [
    "The Union Find can also make use of the weights on a decoding graph to increase its\n",
    "accuracy when decoding errors on a code where the error probabilities are not equal\n",
    "everywhere. A weighted Union Find, as proposed in [Huang, 2021]. Here, they propose to\n",
    "add a small modification to the algorithm, specifically in the way in which edges are\n",
    "grown.\n",
    "\n",
    "As mentioned previously, the original (unweighted) Union Find grows, at each growth\n",
    "step, every vertex connected to the growing-cluster by a measure of a half-edge. In\n",
    "order to make the edges grow based on their weights, we will first find , amongst the\n",
    "edges to-be-grown, the one with the smallest weight, and assign its weight to a\n",
    "variable $ w_{min} $. Then, we will complete the growth of every other edge that\n",
    "shares the same weight as $ w_{min} $. Then, the weight of every other edge will be\n",
    "updated, following:\n",
    "\n",
    "$$\n",
    "w_i \\gets w_i - w_{min}\n",
    "$$\n",
    "\n",
    "We can use `plaquette` to decode a code by using weights. We will be using the same\n",
    "error distribution as the one we obtained for the MWPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e111f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sample = get_sample_random(planar_5, random_qed)\n",
    "weighted_uf = UnionFindDecoder.from_code(planar_5, random_qed, weighted=True)\n",
    "correction = [weighted_uf.decode(sample.erased_qubits, sample.syndrome)]\n",
    "vis_5 = LatticeVisualizer(planar_5)\n",
    "vis_5.draw_latticedata(height=500, syndrome=sample.syndrome[0], correction=correction)  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db2f5a",
   "metadata": {},
   "source": [
    "## Logical error rate\n",
    "\n",
    "One way of determining the decoding accuracy of a decoder is to obtain a *logical error\n",
    "rate*. As its name suggests, this is the rate with which we obtain logical errors. The\n",
    "logical error can vary depending on:\n",
    "\n",
    "- The code.  \n",
    "- The distance of the code.  \n",
    "- The decoding algorithm.  \n",
    "- Whether or not we are using weights.  \n",
    "\n",
    "\n",
    "We can obtain the logical error rate through Monte Carlo simulations, i.e., we choose a\n",
    "number of samples that we want to run, we obtain a correction per each sample, we\n",
    "compare the correction with the error (which is something we can do *in simulations*)\n",
    "to determine if we obtained a logical error, and we count the number of logical errors\n",
    "encountered and divide it by the number of samples. We want this number of samples to\n",
    "be big enough so that it is statistically relevant.\n",
    "\n",
    "We can use the function [`check_success()`](../../apidoc/plaquette.decoders.ipynb#plaquette.decoders.decoderbase.check_success) from `plaquette.decoders.decoderbase`\n",
    "to see whether a correction was successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd385376",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from plaquette.decoders.decoderbase import check_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210156a",
   "metadata": {},
   "source": [
    "Let’s compare the two algorithms, both weighted and unweighted, under the same\n",
    "conditions to see which one has the smallest logical error rate (ergo, the hightest\n",
    "accuracy), and to be made sure that using weights actually improves their accuracy!\n",
    "\n",
    "We will use a planar code of distance 5 and the following non-equal probability\n",
    "distribution that we used to test the weighted MWPM and the weighted Union Find to\n",
    "obtain the logical error rates. Make sure to change the code, the distance of the code\n",
    "and the error probabilities to see how their performance changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2d373",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "planar_5 = LatticeCode.make_planar(size=5, n_rounds=1)\n",
    "plaquette.rnd = np.random.default_rng(seed=1234)  # ensures repeatability of the following results\n",
    "error_probabilities = plaquette.rnd.uniform(0, 0.1, (planar_5.n_data_qubits, 3))\n",
    "random_qed: QubitErrorsDict = {\n",
    "    \"pauli\": {i: dict(x=error_probabilities[i, 0],\n",
    "                      y=error_probabilities[i, 1],\n",
    "                      z=error_probabilities[i, 2]) for i in range(planar_5.n_data_qubits)}\n",
    "}\n",
    "mwpm = PyMatchingDecoder.from_code(planar_5, random_qed, weighted=False)\n",
    "weighted_mwpm = PyMatchingDecoder.from_code(planar_5, random_qed, weighted=True)\n",
    "uf = UnionFindDecoder.from_code(planar_5, random_qed, weighted=False)\n",
    "weighted_uf = UnionFindDecoder.from_code(planar_5, random_qed, weighted=True)\n",
    "successes_mwpm = 0\n",
    "successes_w_mwpm = 0\n",
    "successes_uf = 0\n",
    "successes_w_uf = 0\n",
    "\n",
    "reps = 5000\n",
    "for _ in range(reps):\n",
    "    sample = get_sample_random(planar_5, random_qed)\n",
    "    corr_mwpm = mwpm.decode(sample.erased_qubits, sample.syndrome)\n",
    "    corr_w_mwpm = weighted_mwpm.decode(sample.erased_qubits, sample.syndrome)\n",
    "    corr_uf = uf.decode(sample.erased_qubits, sample.syndrome)\n",
    "    corr_w_uf = weighted_uf.decode(sample.erased_qubits, sample.syndrome)\n",
    "    if check_success(planar_5, corr_mwpm, sample.logical_op_toggle, \"X\"):\n",
    "        successes_mwpm += 1\n",
    "    if check_success(planar_5, corr_w_mwpm, sample.logical_op_toggle, \"X\"):\n",
    "        successes_w_mwpm += 1\n",
    "    if check_success(planar_5, corr_uf, sample.logical_op_toggle, \"X\"):\n",
    "        successes_uf += 1\n",
    "    if check_success(planar_5, corr_w_uf, sample.logical_op_toggle, \"X\"):\n",
    "        successes_w_uf += 1\n",
    "\n",
    "print(\"Error rate of the unweighted MWPM: \", 1 - successes_mwpm / reps)\n",
    "print(\"Error rate of the weighted MWPM: \", 1 - successes_w_mwpm / reps)\n",
    "print(\"Error rate of the unweighted Union Find: \", 1 - successes_uf / reps)\n",
    "print(\"Error rate of the weighted Union Find: \", 1 - successes_w_uf / reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e18ec1",
   "metadata": {},
   "source": [
    "The results show that the decoder algorithms were able to correct the occurring error in\n",
    "approx. 77% of all cases. This means that the correction fails 22% of the times with the\n",
    "given settings.\n",
    "\n",
    "As we can see from these results, the unweighted MWPM has a higher accuracy than the\n",
    "unweighted Union Find decoder (and the weighted MWPM has a higher accuracy than the\n",
    "weighted UF). This is because the Union Find trades decoding accuracy for speed.\n",
    "Remember: the Union Find is almost linear, while the MWPM runs with a complexity\n",
    "higher-than cubic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e90b1e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Knowing that errors have occurred is important, but it is only half of the rent.\n",
    "In this section, we learnt about decoder and how it is used to analyze the error\n",
    "syndromes and infer the specific locations and types of errors that have corrupted\n",
    "our quantum information.\n",
    "\n",
    "Decoding algorithms help determine the most likely error configuration. We have seen\n",
    "how algorithms can utilize graph-based representations that model the relationships\n",
    "between qubits and the stabilizer measurement outcomes, allowing decoders to\n",
    "efficiently process and correct occurring errors. We also learned that a correction can\n",
    "be successful or not, and what are the consequences of having an unsuccessful\n",
    "correction.\n",
    "\n",
    "A correction must be found *extremely* quickly, which is why we introduced two decoding\n",
    "algorithms that are able to find corrections in polynomial time. The MWPM has a higher\n",
    "accuracy than the Union Find, but the Union Find is a *considerably* faster decoder.\n",
    "Depending on the experimental device that you want to protect against errors, it might\n",
    "be more or less convenient to choose one or the other.\n",
    "\n",
    "We also considered a realistic scenario, where each qubit might be subject to a different\n",
    "source of errors with different probabilities, learning how we can make use\n",
    "of this information about our experimental device to improve our decoding accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d91c2",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "- 1. Gimeno-Segovia, “Towards practical linear optical quantum computing,” Nov. 2015. Accepted: 2017-02-02T11:36:32Z Publisher: Imperial College London.  \n",
    "- 1. Dennis, A. Kitaev, A. Landahl, and J. Preskill, “Topological quantum memory,” Journal of Mathematical Physics, vol. 43, pp. 4452–4505, Sept. 2002. Publisher: American Institute of Physics.  \n",
    "- 1. Higgott, “PyMatching: A Python package for decoding quantum codes with minimum-weight perfect matching,” ACM Transactions on Quantum Computing, 2021.  \n",
    "- 1. 1. Dijkstra, “A note on two problems in connexion with graphs,” Numerische Mathematik, vol. 1, pp. 269–271, Dec. 1959.  \n",
    "- 1. Kolmogorov, “Blossom V: a new implementation of a minimum cost perfect matching algorithm,” Mathematical Programming Computation, vol. 1, pp. 43–67, July 2009.  \n",
    "- 1. Delfosse and N. H. Nickerson, “Almost-linear time decoding algorithm for topological codes,” Quantum, vol. 5, p. 595, Dec. 2021. arXiv: 1709.06218.  \n",
    "- 1. Delfosse and G. Zémor, “Linear-time maximum likelihood decoding of surface codes over the quantum erasure channel,” Physical Review Research, vol. 2, p. 033042, July 2020.  \n",
    "- 1. Huang, M. Newman, and K. R. Brown, “Fault-tolerant weighted union-find decoding on the toric code,” Physical Review A, vol. 102, p. 012419, July 2020.  "
   ]
  }
 ],
 "metadata": {
  "date": 1.6860447554035048E9,
  "filename": "decdoing_nb.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "The decoding problem"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
